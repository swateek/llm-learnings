{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import configparser\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from crewai import Agent, Task, Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILENAME=\".config.ini\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_FILENAME)\n",
    "\n",
    "def get_value_by_section_and_key(section, key):\n",
    "        \"\"\"get_value_by_section_and_key\"\"\"\n",
    "        return config.get(section, key)\n",
    "\n",
    "def get_all_details_of_section(section) -> dict:\n",
    "    \"\"\"get_all_details_of_section\"\"\"\n",
    "    return dict(config.items(section))\n",
    "\n",
    "\n",
    "default_cfgs = get_all_details_of_section(\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai_api_key = default_cfgs[\"openai_api_key\"]\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4o-mini'\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tasks and Agents YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for YAML configurations\n",
    "files = {\n",
    "    'lead_agents': 'config/lead_qualification_agents.yaml',\n",
    "    'lead_tasks': 'config/lead_qualification_tasks.yaml',\n",
    "    'email_agents': 'config/email_engagement_agents.yaml',\n",
    "    'email_tasks': 'config/email_engagement_tasks.yaml'\n",
    "}\n",
    "\n",
    "# Load configurations from YAML files\n",
    "configs = {}\n",
    "for config_type, file_path in files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        configs[config_type] = yaml.safe_load(file)\n",
    "\n",
    "# Assign loaded configurations to specific variables\n",
    "lead_agents_config = configs['lead_agents']\n",
    "lead_tasks_config = configs['lead_tasks']\n",
    "email_agents_config = configs['email_agents']\n",
    "email_tasks_config = configs['email_tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pydantic Models for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, Optional, List, Set, Tuple\n",
    "\n",
    "class LeadPersonalInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"The full name of the lead.\")\n",
    "    job_title: str = Field(..., description=\"The job title of the lead.\")\n",
    "    role_relevance: int = Field(..., ge=0, le=10, description=\"A score representing how relevant the lead's role is to the decision-making process (0-10).\")\n",
    "    professional_background: Optional[str] = Field(..., description=\"A brief description of the lead's professional background.\")\n",
    "\n",
    "class CompanyInfo(BaseModel):\n",
    "    company_name: str = Field(..., description=\"The name of the company the lead works for.\")\n",
    "    industry: str = Field(..., description=\"The industry in which the company operates.\")\n",
    "    company_size: int = Field(..., description=\"The size of the company in terms of employee count.\")\n",
    "    revenue: Optional[float] = Field(None, description=\"The annual revenue of the company, if available.\")\n",
    "    market_presence: int = Field(..., ge=0, le=10, description=\"A score representing the company's market presence (0-10).\")\n",
    "\n",
    "class LeadScore(BaseModel):\n",
    "    score: int = Field(..., ge=0, le=100, description=\"The final score assigned to the lead (0-100).\")\n",
    "    scoring_criteria: List[str] = Field(..., description=\"The criteria used to determine the lead's score.\")\n",
    "    validation_notes: Optional[str] = Field(None, description=\"Any notes regarding the validation of the lead score.\")\n",
    "\n",
    "class LeadScoringResult(BaseModel):\n",
    "    personal_info: LeadPersonalInfo = Field(..., description=\"Personal information about the lead.\")\n",
    "    company_info: CompanyInfo = Field(..., description=\"Information about the lead's company.\")\n",
    "    lead_score: LeadScore = Field(..., description=\"The calculated score and related information for the lead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead Qualification Crew, Agents and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Agents\n",
    "lead_data_agent = Agent(\n",
    "  config=lead_agents_config['lead_data_agent'],\n",
    "  tools=[SerperDevTool(), ScrapeWebsiteTool()]\n",
    ")\n",
    "\n",
    "cultural_fit_agent = Agent(\n",
    "  config=lead_agents_config['cultural_fit_agent'],\n",
    "  tools=[SerperDevTool(), ScrapeWebsiteTool()]\n",
    ")\n",
    "\n",
    "scoring_validation_agent = Agent(\n",
    "  config=lead_agents_config['scoring_validation_agent'],\n",
    "  tools=[SerperDevTool(), ScrapeWebsiteTool()]\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "lead_data_task = Task(\n",
    "  config=lead_tasks_config['lead_data_collection'],\n",
    "  agent=lead_data_agent\n",
    ")\n",
    "\n",
    "cultural_fit_task = Task(\n",
    "  config=lead_tasks_config['cultural_fit_analysis'],\n",
    "  agent=cultural_fit_agent\n",
    ")\n",
    "\n",
    "scoring_validation_task = Task(\n",
    "  config=lead_tasks_config['lead_scoring_and_validation'],\n",
    "  agent=scoring_validation_agent,\n",
    "  context=[lead_data_task, cultural_fit_task],\n",
    "  output_pydantic=LeadScoringResult\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "lead_scoring_crew = Crew(\n",
    "  agents=[\n",
    "    lead_data_agent,\n",
    "    cultural_fit_agent,\n",
    "    scoring_validation_agent\n",
    "  ],\n",
    "  tasks=[\n",
    "    lead_data_task,\n",
    "    cultural_fit_task,\n",
    "    scoring_validation_task\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Engagement Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Agents\n",
    "email_content_specialist = Agent(\n",
    "  config=email_agents_config['email_content_specialist']\n",
    ")\n",
    "\n",
    "engagement_strategist = Agent(\n",
    "  config=email_agents_config['engagement_strategist']\n",
    ")\n",
    "\n",
    "# Creating Tasks\n",
    "email_drafting = Task(\n",
    "  config=email_tasks_config['email_drafting'],\n",
    "  agent=email_content_specialist\n",
    ")\n",
    "\n",
    "engagement_optimization = Task(\n",
    "  config=email_tasks_config['engagement_optimization'],\n",
    "  agent=engagement_strategist\n",
    ")\n",
    "\n",
    "# Creating Crew\n",
    "email_writing_crew = Crew(\n",
    "  agents=[\n",
    "    email_content_specialist,\n",
    "    engagement_strategist\n",
    "  ],\n",
    "  tasks=[\n",
    "    email_drafting,\n",
    "    engagement_optimization\n",
    "  ],\n",
    "  verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Complete Sales Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    @start()\n",
    "    def fetch_leads(self):\n",
    "        # Pull our leads from the database\n",
    "        leads = [\n",
    "            {\n",
    "                \"lead_data\": {\n",
    "                    \"name\": \"João Moura\",\n",
    "                    \"job_title\": \"Director of Engineering\",\n",
    "                    \"company\": \"Clearbit\",\n",
    "                    \"email\": \"joao@clearbit.com\",\n",
    "                    \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "        return leads\n",
    "\n",
    "    @listen(fetch_leads)\n",
    "    def score_leads(self, leads):\n",
    "        scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "        self.state[\"score_crews_results\"] = scores\n",
    "        return scores\n",
    "\n",
    "    @listen(score_leads)\n",
    "    def store_leads_score(self, scores):\n",
    "        # Here we would store the scores in the database\n",
    "        return scores\n",
    "\n",
    "    @listen(score_leads)\n",
    "    def filter_leads(self, scores):\n",
    "        return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "    @listen(filter_leads)\n",
    "    def write_email(self, leads):\n",
    "        scored_leads = [lead.to_dict() for lead in leads]\n",
    "        emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "        return emails\n",
    "\n",
    "    @listen(write_email)\n",
    "    def send_email(self, emails):\n",
    "        # Here we would send the emails to the leads\n",
    "        return emails\n",
    "\n",
    "flow = SalesPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./crewai_flow.html', width='150%', height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Kickoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = await flow.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Metrics and Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([flow.state[\"score_crews_results\"][0].token_usage.dict()])\n",
    "\n",
    "# Calculate total costs\n",
    "costs = 0.150 * df_usage_metrics['total_tokens'].sum() / 1_000_000\n",
    "print(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([emails[0].token_usage.dict()])\n",
    "\n",
    "# Calculate total costs\n",
    "costs = 0.150 * df_usage_metrics['total_tokens'].sum() / 1_000_000\n",
    "print(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = flow.state[\"score_crews_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "lead_scoring_result = scores[0].pydantic\n",
    "\n",
    "# Create a dictionary with the nested structure flattened\n",
    "data = {\n",
    "    'Name': lead_scoring_result.personal_info.name,\n",
    "    'Job Title': lead_scoring_result.personal_info.job_title,\n",
    "    'Role Relevance': lead_scoring_result.personal_info.role_relevance,\n",
    "    'Professional Background': lead_scoring_result.personal_info.professional_background,\n",
    "    'Company Name': lead_scoring_result.company_info.company_name,\n",
    "    'Industry': lead_scoring_result.company_info.industry,\n",
    "    'Company Size': lead_scoring_result.company_info.company_size,\n",
    "    'Revenue': lead_scoring_result.company_info.revenue,\n",
    "    'Market Presence': lead_scoring_result.company_info.market_presence,\n",
    "    'Lead Score': lead_scoring_result.lead_score.score,\n",
    "    'Scoring Criteria': ', '.join(lead_scoring_result.lead_score.scoring_criteria),\n",
    "    'Validation Notes': lead_scoring_result.lead_score.validation_notes\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns=['Value'])\n",
    "\n",
    "# Reset the index to turn the original column names into a regular column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Rename the index column to 'Attribute'\n",
    "df = df.rename(columns={'index': 'Attribute'})\n",
    "\n",
    "# Create HTML table with bold attributes and left-aligned values\n",
    "html_table = df.style.set_properties(**{'text-align': 'left'}) \\\n",
    "                     .format({'Attribute': lambda x: f'<b>{x}</b>'}) \\\n",
    "                     .hide(axis='index') \\\n",
    "                     .to_html()\n",
    "\n",
    "# Display the styled HTML table\n",
    "display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "result_text = emails[0].raw\n",
    "wrapped_text = textwrap.fill(result_text, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Complex Can it Get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Flow\n",
    "from crewai.flow.flow import listen, start, and_, or_, router\n",
    "\n",
    "class SalesPipeline(Flow):\n",
    "    \n",
    "  @start()\n",
    "  def fetch_leads(self):\n",
    "    # Pull our leads from the database\n",
    "    # This is a mock, in a real-world scenario, this is where you would\n",
    "    # fetch leads from a database\n",
    "    leads = [\n",
    "      {\n",
    "        \"lead_data\": {\n",
    "          \"name\": \"João Moura\",\n",
    "          \"job_title\": \"Director of Engineering\",\n",
    "          \"company\": \"Clearbit\",\n",
    "          \"email\": \"joao@clearbit.com\",\n",
    "          \"use_case\": \"Using AI Agent to do better data enrichment.\"\n",
    "        },\n",
    "      },\n",
    "    ]\n",
    "    return leads\n",
    "\n",
    "  @listen(fetch_leads)\n",
    "  def score_leads(self, leads):\n",
    "    scores = lead_scoring_crew.kickoff_for_each(leads)\n",
    "    self.state[\"score_crews_results\"] = scores\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def store_leads_score(self, scores):\n",
    "    # Here we would store the scores in the database\n",
    "    return scores\n",
    "\n",
    "  @listen(score_leads)\n",
    "  def filter_leads(self, scores):\n",
    "    return [score for score in scores if score['lead_score'].score > 70]\n",
    "\n",
    "  @listen(and_(filter_leads, store_leads_score))\n",
    "  def log_leads(self, leads):\n",
    "    print(f\"Leads: {leads}\")\n",
    "\n",
    "  @router(filter_leads, paths=[\"high\", \"medium\", \"low\"])\n",
    "  def count_leads(self, scores):\n",
    "    if len(scores) > 10:\n",
    "      return 'high'\n",
    "    elif len(scores) > 5:\n",
    "      return 'medium'\n",
    "    else:\n",
    "      return 'low'\n",
    "\n",
    "  @listen('high')\n",
    "  def store_in_salesforce(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('medium')\n",
    "  def send_to_sales_team(self, leads):\n",
    "    return leads\n",
    "\n",
    "  @listen('low')\n",
    "  def write_email(self, leads):\n",
    "    scored_leads = [lead.to_dict() for lead in leads]\n",
    "    emails = email_writing_crew.kickoff_for_each(scored_leads)\n",
    "    return emails\n",
    "\n",
    "  @listen(write_email)\n",
    "  def send_email(self, emails):\n",
    "    # Here we would send the emails to the leads\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = SalesPipeline()\n",
    "flow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./crewai_flow_complex.html', width='150%', height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
