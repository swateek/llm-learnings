{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWoGOis4KoG"
   },
   "source": [
    "# Lesson 6: Metaprompting with o1\n",
    "\n",
    "Optimizing to production is one of the biggest painpoints we've seen developers experience with working with LLMs - with so much guidance for prompt engineering, RAG and fine-tuning out there, figuring out which optimization you need to hill-climb on your evals can be a difficult problem to frame and solve.\n",
    "\n",
    "Luckily, it appears to be one of the use cases that `o1` is capable at. In this session we'll focus on how to use `o1-mini` to work with a set of evals to optimize our prompt for the task and improve score on our evals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import OpenAI key\n",
    "from helper import get_openai_api_key\n",
    "openai_api_key = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functionDefinitions import TOOLS\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GPT_MODEL = 'gpt-4o-mini'\n",
    "O1_MODEL = 'o1'\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redlines import Redlines\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def compare_strings(string1, string2):\n",
    "    # Create a Redlines object to compare the strings\n",
    "    diff = Redlines(string1, string2)\n",
    "    \n",
    "    # Display the differences using Markdown\n",
    "    display(Markdown(diff.output_markdown))\n",
    "\n",
    "# Example usage\n",
    "string_a = \"This is the original text for comparison purposes.\"\n",
    "string_b = \"This is the modified text to compare for differences.\"\n",
    "\n",
    "compare_strings(string_a, string_b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate 4o routine (prompt)\n",
    "\n",
    "We'll take the Flight Cancellation Policy that we have created and convert it to an LLM-based routine with the following prompt.\n",
    "\n",
    "This process is also a good use of O1, using a purely text document to create instructions and tools for an LLM to make use of. To dive deeper on this topic check out [this cookbook](https://cookbook.openai.com/examples/o1/using_reasoning_for_routine_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('originalPolicy/flightCancellationsPolicy.md', 'r') as file:\n",
    "    flight_cancellation_policy = file.read()\n",
    "    print(flight_cancellation_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION_PROMPT = f\"\"\"\n",
    "You are a helpful assistant tasked with taking an external facing help center article and converting it into a internal-facing programmatically executable routine optimized for an LLM. \n",
    "The LLM using this routine will be tasked with reading the policy, answering incoming questions from customers, and helping drive the case toward resolution.\n",
    "\n",
    "Please follow these instructions:\n",
    "1. **Review the customer service policy carefully** to ensure every step is accounted for. It is crucial not to skip any steps or policies.\n",
    "2. **Organize the instructions into a logical, step-by-step order**, using the specified format. \n",
    "3. **Use the following format**:\n",
    "   - **Main actions are numbered** (e.g., 1, 2, 3).\n",
    "   - **Sub-actions are lettered** under their relevant main actions (e.g., 1a, 1b).\n",
    "      **Sub-actions should start on new lines**\n",
    "   - **Specify conditions using clear 'if...then...else' statements** (e.g., 'If the product was purchased within 30 days, then...').\n",
    "   - **For instructions that require more information from the customer**, provide polite and professional prompts to ask for additional information.\n",
    "   - **For actions that require data from external systems**, write a step to call a function using backticks for the function name (e.g., call the `check_delivery_date` function).\n",
    "      - **If a step requires the customer service agent to take an action** (e.g., process a refund), generate a function call for this action (e.g., call the `process_refund` function).\n",
    "      - **Only use the available set of functions that are defined below.\n",
    "   - **If there is an action an assistant can perform on behalf of the user**, include a function call for this action (e.g., call the `change_email_address` function), and ensure the function is defined with its purpose and required parameters.\n",
    "      - **Only use the available set of functions that are defined below.\n",
    "   - **The step prior to case resolution should always be to ask if there is anything more you can assist with**.\n",
    "   - **End with a final action for case resolution**: calling the `case_resolution` function should always be the final step.\n",
    "4. **Ensure compliance** by making sure all steps adhere to company policies, privacy regulations, and legal requirements.\n",
    "5. **Handle exceptions or escalations** by specifying steps for scenarios that fall outside the standard policy.\n",
    "6. **Ensure coverage** by checking that all of the conditions covered in the policy are also covered in the routines\n",
    "\n",
    "**Important**: Always wrap the functions you return in backticks i.e. `check_ticket_type`. Do not include the arguments to the functions.\n",
    "\n",
    "Here are the currently available set of functions in JSON format: \n",
    "TOOLS: {TOOLS}\n",
    "\n",
    "Please convert the following customer service policy into the formatted routine, ensuring it is easy to follow and execute programmatically. Ensure that you **only** use the functions provided and do **not** create net new functions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_routine(policy):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                    {CONVERSION_PROMPT}\n",
    "\n",
    "                    POLICY:\n",
    "                    {policy}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model='o1-mini',\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "\n",
    "        return response.choices[0].message.content \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_cancellation_routine = generate_routine(flight_cancellation_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(flight_cancellation_routine))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality check\n",
    "\n",
    "We'll check that `o1-mini` has followed our instructions and used only the functions we provided.\n",
    "\n",
    "If it added additional ones, we'll need to write functions for them before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def compare_lists(a, b):\n",
    "    # Create Counters for both lists\n",
    "    counter_a = Counter(a)\n",
    "    counter_b = Counter(b)\n",
    "    \n",
    "    # Find elements that are only in A or have a higher count in A\n",
    "    diff_a = counter_a - counter_b\n",
    "    # Find elements that are only in B or have a higher count in B\n",
    "    diff_b = counter_b - counter_a\n",
    "    \n",
    "    # Report differences\n",
    "    print(\"Items in A that are not in B (or more of in A):\")\n",
    "    for item, count in diff_a.items():\n",
    "        print(f\"{item}: {count}\")\n",
    "\n",
    "    print(\"\\nItems in B that are not in A (or more of in B):\")\n",
    "    for item, count in diff_b.items():\n",
    "        print(f\"{item}: {count}\")\n",
    "\n",
    "def extract_function_names(text):\n",
    "    # Use regex to match text between backticks and extract function names\n",
    "    pattern = r'`(.*?)`'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Extract a unique list of the functions in the generated routine\n",
    "function_names_from_o1 = set(extract_function_names(flight_cancellation_routine))\n",
    "\n",
    "# Extract the list of TOOLs we provided the model\n",
    "function_names_defined = [tool[\"function\"][\"name\"] for tool in TOOLS if tool[\"type\"] == \"function\"]\n",
    "\n",
    "# Print the differences\n",
    "## Items that are in A and not in B need functions written for them\n",
    "## Items that are in B and not in A are fine, they are just unused in the routine the model has written\n",
    "compare_lists(function_names_from_o1, function_names_defined)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation\n",
    "\n",
    "Now that we have a routine generated with o1, we can run it against our evaluation suite and measure its accuracy.\n",
    "\n",
    "We'll start by creating an agent that is equipped with the policy and a list of tools. It will be given messages from an existing conversation and will be tasked with determining the next best action to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_response(transcript, policy, model):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"\n",
    "You are a customer service agent that is responsible for handling airline related issues. Below is the exact policy that you must follow to address the customer's issue.\n",
    "\n",
    "POLICY:\n",
    "{policy}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        messages.extend(transcript)\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=TOOLS,\n",
    "            parallel_tool_calls=False,\n",
    "        temperature=0)\n",
    "        \n",
    "        return response.choices[0].message \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process each row in parallel to reduce runtime and compare the function call + inputs that the model selects against our expected function + parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def safe_str(value):\n",
    "    return \"\" if (isinstance(value, float) and math.isnan(value)) else str(value)\n",
    "\n",
    "def process_row(row_number,test_row,policy,model,iteration=0, verbose=False):\n",
    "    # Extract variables from test_row\n",
    "    context = test_row['context']\n",
    "    request = test_row['request']\n",
    "    full_name = test_row['full_name']\n",
    "    booking_ref = test_row['booking_ref']\n",
    "    flight_no = test_row['flight_no']\n",
    "    ticket_type = test_row['ticket_type']\n",
    "    fare_rules = safe_str(test_row['fare_rules'])\n",
    "    medical_certificate = safe_str(test_row['medical_certificate'])\n",
    "    refund_amount = safe_str(test_row['refund_amount'])\n",
    "    expected_function = test_row['expected_function']\n",
    "    expected_inputs_str = test_row['expected_inputs']\n",
    "    expected_inputs = json.loads(expected_inputs_str)\n",
    "\n",
    "    CUSTOMER_PROMPT = f\"\"\"<objective>You are a customer named {full_name} trying to resolve an issue with a customer service agent.\n",
    "    Continue providing the information the agent requests so they can solve your issue.\n",
    "    If you have the information in your *details* then use that. Otherwise, generate an appropriate answer.</objective>\n",
    "    <details>\n",
    "    Booking Reference: {booking_ref}\n",
    "    Flight Number: {flight_no}\n",
    "    {f\"Medical Reference: {medical_certificate}\" if medical_certificate else \"\"}\n",
    "    Context: {context}\n",
    "    Your initial request: {request}\n",
    "    </details>\n",
    "    <guidance>\n",
    "    Remember that YOU are the customer.\n",
    "    If the agent needs some information, that must come from you. \n",
    "    Do not ask the agent for information.\n",
    "    </guidance>\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize transcript\n",
    "    transcript = [{\"role\": \"user\", \"content\": request}]\n",
    "    if verbose:\n",
    "        print(f'Starting request: {request}')\n",
    "    \n",
    "    # Initialize variables\n",
    "    is_correct = False\n",
    "\n",
    "    loop_count = 0\n",
    "    while True:\n",
    "        loop_count += 1\n",
    "        if loop_count > 10:\n",
    "            actual_function = None\n",
    "            actual_inputs = None\n",
    "            break\n",
    "        # Call agent_response with the current transcript, policy, model\n",
    "        response = agent_response(transcript, policy, GPT_MODEL)\n",
    "        \n",
    "        # Extract assistant message content and tool_calls\n",
    "        if response:\n",
    "            assistant_message_content = response.content\n",
    "            tool_calls = response.tool_calls\n",
    "        else:\n",
    "            assistant_message_content = None\n",
    "            tool_calls = None\n",
    "        \n",
    "        if not tool_calls:\n",
    "            # Append assistant's message to transcript\n",
    "            transcript.append({\"role\": \"assistant\", \"content\": assistant_message_content})\n",
    "            \n",
    "            # Get customer response\n",
    "            customer_messages = [{\"role\": \"system\", \"content\": CUSTOMER_PROMPT}]\n",
    "            customer_messages.extend(transcript)\n",
    "            \n",
    "            customer_response = client.chat.completions.create(model=GPT_MODEL, messages=customer_messages)\n",
    "            \n",
    "            # Append customer response to transcript\n",
    "            transcript.append({\"role\": \"user\", \"content\": customer_response.choices[0].message.content})\n",
    "            \n",
    "            # Continue the loop\n",
    "            continue\n",
    "        else:\n",
    "            #print(tool_calls)\n",
    "            # There is a tool call\n",
    "            tool_call = tool_calls[0]  # Assuming we're only interested in the first tool call\n",
    "            function_name = tool_call.function.name\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name == 'verify_identity':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                # Simulate the tool response\n",
    "                tool_response = \"True\"\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'ask_clarification':\n",
    "                # Take the 'prompt' from arguments and use it as assistant message\n",
    "                clarification_prompt = arguments.get('prompt')\n",
    "                \n",
    "                # Append assistant's clarification message\n",
    "                transcript.append({\"role\": \"assistant\", \"content\": clarification_prompt})\n",
    "                \n",
    "                # Get customer response\n",
    "                customer_messages = [{\"role\": \"system\", \"content\": CUSTOMER_PROMPT}]\n",
    "                customer_messages.extend(transcript)\n",
    "                \n",
    "                customer_response = client.chat.completions.create(model=GPT_MODEL, messages=customer_messages)\n",
    "                \n",
    "                # Append customer response to transcript\n",
    "                transcript.append({\"role\": \"user\", \"content\": customer_response.choices[0].message.content})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'check_ticket_type':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                tool_response = f'Ticket type: {ticket_type}\\nFare rules: {fare_rules}'\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'check_fare_rules':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                tool_response = f'Ticket type: {ticket_type}\\nFare rules: {fare_rules}'\n",
    "\n",
    "                if fare_rules == 'partial_refund':\n",
    "                    tool_response += f'\\nRefund amount: {refund_amount}'\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'get_refund_amount':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                tool_response = f'Refund amount: {refund_amount}'\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'check_next_available_flight':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                # simulating the tool response\n",
    "                tool_response = f'Next available flight: LMG091 at 10:00 AM Tomorrow'\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            elif function_name == 'provide_alternative_options':\n",
    "                # Append assistant's message to transcript\n",
    "                transcript.append(response)\n",
    "                \n",
    "                # simulating the tool response\n",
    "                tool_response = f'Next available flight: FR9876 at 5:00 PM Tomorrow'\n",
    "                \n",
    "                # Append the tool response to transcript\n",
    "                transcript.append({\"role\": \"tool\", \"content\": tool_response, \"tool_call_id\": tool_call.id})\n",
    "                \n",
    "                # Continue the loop\n",
    "                continue\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Got a successful function call')\n",
    "                # Stop and compare function call and arguments with expected_function and expected_inputs\n",
    "                transcript.append(response)\n",
    "                actual_function = function_name\n",
    "                actual_inputs = arguments\n",
    "                is_correct = (actual_function == expected_function) and (str(actual_inputs) == str(expected_inputs))\n",
    "                \n",
    "                # Break the loop\n",
    "                break\n",
    "    \n",
    "    # Return the results\n",
    "    return {\n",
    "        'request': request,\n",
    "        'context': context,\n",
    "        'expected_function': expected_function,\n",
    "        'expected_inputs': expected_inputs,\n",
    "        'actual_function': actual_function,\n",
    "        'actual_inputs': actual_inputs,\n",
    "        'is_correct': is_correct,\n",
    "        'transcript': transcript\n",
    "    }\n",
    "\n",
    "# added post-filming to address transcript size\n",
    "def filter_messages(message_list):\n",
    "    messages = []\n",
    "    for item in message_list:\n",
    "        # Convert the item to a string and check if 'ChatCompletionMessage' is in it.\n",
    "        if 'ChatCompletionMessage' in str(item):\n",
    "            #print(item)\n",
    "            message = {\"role\": \"assistant\",\n",
    "                       \"tool_call\": {\n",
    "                           \"name\": item.tool_calls[0].function.name,\n",
    "                           \"arguments\": item.tool_calls[0].function.arguments\n",
    "                       }\n",
    "                      }\n",
    "            messages.append(message)\n",
    "        else:\n",
    "            messages.append(item)\n",
    "    return messages\n",
    "\n",
    "\n",
    "def evaluate_function_calls(df, policy, model, i=0, verbose=False):\n",
    "    records = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to process rows in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_row, row_number, row, policy, model, i, verbose): row_number for row_number, row in df.iterrows()}\n",
    "        for future in futures:\n",
    "            record = future.result()\n",
    "            records.append(record)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df['cleaned_transcript'] = df['transcript'].apply(filter_messages)\n",
    "    total_accuracy = df['is_correct'].mean()\n",
    "    return df, total_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('evals/policyEvals.csv')\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def display_row(index):\n",
    "    eval_row = eval_df.iloc[index]\n",
    "    df_row = df.iloc[index][[\"actual_function\", \"actual_inputs\", \"is_correct\", \"cleaned_transcript\"]]\n",
    "    transcript_list = df_row[\"cleaned_transcript\"]\n",
    "\n",
    "    print(\"Eval DataFrame Row:\")\n",
    "    print(eval_row)\n",
    "    print(\"\\nDataFrame Columns:\")\n",
    "    print(\"actual_function:\", df_row[\"actual_function\"])\n",
    "    print(\"actual_inputs:\", df_row[\"actual_inputs\"])\n",
    "    print(\"is_correct:\", df_row[\"is_correct\"])\n",
    "    print(\"transcript:\")\n",
    "    for entry in transcript_list:\n",
    "        role = entry[\"role\"] if isinstance(entry, dict) else getattr(entry, \"role\", None)\n",
    "        color = \"blue\" if role == \"user\" else \"red\"\n",
    "        print(colored(str(entry), color))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, accuracy = evaluate_function_calls(eval_df, flight_cancellation_routine, GPT_MODEL)\n",
    "\n",
    "# Display the accuracy as a mini header\n",
    "display(Markdown(f\"### Accuracy: {accuracy:.2%}\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_row(8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Improve 4o routine\n",
    "\n",
    "Let's now leverage o1 again to add in a metaprompting loop to see if we can improve the quality of our evals.\n",
    "\n",
    "We'll take the following multi-step approach:\n",
    "- We'll pass in the current routine + eval results to o1 and ask it analyze the results and update the routine accordingly\n",
    "- Since o1 does not currently support structured outputs, we'll chain with output with a 4o to enforce a schema we can parse\n",
    "- Finally, we take the new routine and run it back through our eval to generate new results\n",
    "\n",
    "We'll run this loop a fixed number of times and see what improvements we can make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(messages, model, response_format=None):\n",
    "    try:\n",
    "        if response_format:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                response_format={\"type\": \"json_schema\", \"json_schema\": response_format}\n",
    "            )\n",
    "        else:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "OUTPUT_SCHEMA = {\n",
    "                    \"name\": \"policy_output\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"final_answer\": { \"type\": \"string\" }\n",
    "                    },\n",
    "                    \"required\": [\"final_answer\"],\n",
    "                    \"additionalProperties\": False\n",
    "                    },\n",
    "                    \"strict\": True\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_o1_message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"# Instructions\n",
    "You are an agent that is responsible for improving the quality of instructions that are provided to a customer service LLM agent. \n",
    "Your task is to improve the instructions that are provided to the LLM agent in order to increase accuracy on a test set while adhering to the initial policy. \n",
    "\n",
    "## Criteria\n",
    "- Analyze the existing instructions and the results of the eval. Understand which behaviors lead to failures.\n",
    "- For example, if the LLM agent is not asking for the booking reference when it is needed, you should add a step to ask for the booking reference.\n",
    "- Improve the instructions to address the gaps in the eval results.\n",
    "- Ensure changes made are compliant with the original policy.\n",
    "- Only use the tools provided.\n",
    "- Use the functions provided to the best of your ability to ensure the LLM agent can handle the customer service requests effectively. Including gathering data where necessary.\n",
    "- Try changing the format if this formatting doesn't work well - consider basic XML (e.g. <step> <substep> <if> <case>) or markdown as alternatives.\n",
    "\n",
    "You will be provided with 4 items:\n",
    "1) The ground-truth policy for the customer service agent containing detailed instructions on how to handle flight cancellations and changes.\n",
    "2) Full list of available functions.\n",
    "3) A routine instruction set.\n",
    "4) A results that shows the LLMs performance on a test set using this routine instruction set. This dataset contains columns showing:\n",
    "    - request: This is the initial user request. \n",
    "    - expected_function: This is the function we expect the LLM to call at the end of the conversation. \n",
    "    - expected_input: This is the input we expect the LLM to provide to the function at the end of the conversation.\n",
    "    - actual_function: This is the final function the LLM called using the current instructions.\n",
    "    - actual_input: These are the function parameters the LLM provided based on the current instructions.\n",
    "    - transcript: This is the conversation transcript between the user and the LLM agent. \n",
    "    - is_correct: True/False value depending on if the model responded correctly\n",
    "\n",
    "You may be provided with a history of edits and evaluations. You can use this information to understand what has been tried before and what has worked or not worked.\n",
    "\n",
    "# Data\n",
    "\n",
    "## 1. Original policy\n",
    "{flight_cancellation_policy}\n",
    "\n",
    "## 2. Functions\n",
    "{TOOLS}\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Return the improved policy exactly as written within the defined JSON. Remove all parts from the LLM's answer that are not part of the policy, and do not create additional keys.\n",
    "\"\"\"\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "def num_tokens_from_messages(messages):\n",
    "    entire_input = \"\"\n",
    "    for message in messages:\n",
    "        entire_input += message[\"content\"] + \" \"\n",
    "    tokens = encoding.encode(entire_input)\n",
    "    return len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When improving the routine, consider these approaches:\n",
    "- Adding synthetic examples to show the model how to choose between two commonly mistaken functions\n",
    "- Given it general guidance on what attributes to look out for which it commonly mislabels\n",
    "- Being more specific with your guidance on how to follow the routine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>High Token Warning!:</b> Running the following cell consumes many tokens. Running multiple times may result in a message regarding exceeding a monthy quota.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with data from first run\n",
    "routines = [flight_cancellation_routine]\n",
    "results = [df]\n",
    "accuracies = [round(accuracy * 100, 2)]\n",
    "o1_messages = start_o1_message.copy()\n",
    "\n",
    "MAX_TOKENS = 120000\n",
    "\n",
    "for i in range(3):\n",
    "    display(Markdown(f\"## Iteration {i+1}\"))\n",
    "    \n",
    "    # Build a candidate message with the most recent eval (full table)\n",
    "    prev_results = results[-1]\n",
    "    pf = prev_results.drop(columns=['transcript'])\n",
    "\n",
    "    new_eval_content = (\n",
    "        f\"## 3. Routine instructions:\\n\"\n",
    "        f\"{routines[-1]}\\n\"\n",
    "        f\"## 4. Results table:\\n\"\n",
    "        f\"{pf.to_json(orient='records')}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Check token count before adding the newest eval\n",
    "    candidate_messages = o1_messages + [{\"role\": \"user\", \"content\": new_eval_content}]\n",
    "    if num_tokens_from_messages(candidate_messages) <= MAX_TOKENS:\n",
    "        o1_messages.append({\"role\":\"user\", \"content\": new_eval_content})\n",
    "    else:\n",
    "        # If we can't even fit the newest eval, consider truncation or error handling\n",
    "        # For simplicity, we skip truncation logic here\n",
    "        print(\"excluding eval content {num_tokens_from_messages(candidate_messages)}\")\n",
    "        pass\n",
    "\n",
    "    print(f\"num tokens {num_tokens_from_messages(o1_messages)}\")\n",
    "    print(f\"len messages {len(o1_messages)}\")\n",
    "\n",
    "\n",
    "    # Get the updated routine from the metaprompting assistant\n",
    "    temp_routine_json = get_openai_response(o1_messages\n",
    "                                            , O1_MODEL\n",
    "                                            ,response_format=OUTPUT_SCHEMA)\n",
    "    #temp_routine_str = temp_routine_json.strip(\"json```\").strip(\"```\")\n",
    "    new_routine = json.loads(temp_routine_json)[\"final_answer\"]\n",
    "    routines.append(new_routine)\n",
    "\n",
    "    eval_df = pd.read_csv('evals/policyEvals.csv')\n",
    "\n",
    "    # Evaluate the function calls with the current policy on the gpt-4o model\n",
    "    eval_df, accuracy = evaluate_function_calls(\n",
    "        eval_df,\n",
    "        new_routine,\n",
    "        GPT_MODEL,\n",
    "        i\n",
    "    )\n",
    "    accuracies.append(round(accuracy * 100, 2))\n",
    "    results.append(eval_df)\n",
    "    display(Markdown(f\"### Accuracy: {accuracy:.2%}\"))\n",
    "\n",
    "    #list the IDs of the failed rows\n",
    "    failed_ids = eval_df[eval_df['is_correct'] == False].index.tolist()\n",
    "    display(eval_df.loc[failed_ids])\n",
    "    eval_df.to_csv(f'evals/results_run_{i+1}.csv', index=False)\n",
    "\n",
    "    # Instantiate fresh o1 message\n",
    "    o1_messages = start_o1_message.copy()\n",
    "\n",
    "    # Attempt to include older examples (omitting 'transcript') within token limit\n",
    "    older_results = results[:-1][::-1]  # all except the newest, reversed for most recent first\n",
    "    for old_eval_df in older_results:\n",
    "        if 'transcript' in old_eval_df.columns:\n",
    "            truncated_df = old_eval_df.drop(columns=['transcript','cleaned_transcript'])\n",
    "        else:\n",
    "            truncated_df = old_eval_df\n",
    "\n",
    "        old_content = \"## Older eval (no transcript):\\n\" + truncated_df.to_json(orient='records')\n",
    "        candidate_messages = o1_messages + [{\"role\": \"user\", \"content\": old_content}]\n",
    "        if num_tokens_from_messages(candidate_messages) <= MAX_TOKENS:\n",
    "            o1_messages.append({\"role\": \"user\", \"content\": old_content})\n",
    "        else:\n",
    "            # If we exceed token limits, stop adding older examples\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy over runs\n",
    "plt.plot(range(1,len(accuracies)+1), accuracies, marker='o')\n",
    "plt.title('Accuracy over time')\n",
    "plt.xlabel('Run number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('metaprompt_accuracy.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best routine\n",
    "best_routine = routines[accuracies.index(max(accuracies))]\n",
    "display(Markdown(f\"## Best Routine\\n{best_routine}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routines[accuracies.index(max(accuracies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
